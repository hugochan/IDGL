# Data
data_type: 'text'
dataset_name: 'mrd'
data_dir: '../data/mrd/'
pretrained_word_embed_file: 'your_path_to_glove_vec/glove.840B.300d.txt'
pretrained: null
task_type: 'regression'

# Output
out_dir: '../out/mrd/idgl_anchor'



data_seed: 1234 # Fixed
seed: 1234


# Model architecture
model_name: 'TextGraphRegression'

# Scalable graph learning
scalable_run: True
ratio_anchors: 0.4 # 0.4!


hidden_size: 64 # 64!


# Bert configure
use_bert: False



# Regularization
dropout: 0.5
gl_dropout: 0. # IGL: 0!


# Graph neural networks
bignn: False
graph_module: 'gcn'
graph_type: 'dynamic'
graph_learn: True
graph_metric_type: 'weighted_cosine' # weighted_cosine, kernel, attention, gat_attention, cosine
graph_skip_conn: 0.5 # IGL: 0.5 !
update_adj_ratio: 0.75 # IGL: 0.75!
graph_include_self: False # cosine-KNN-GCN: False
graph_learn_regularization: True
smoothness_ratio: 0.2 # IGL: 0.2!
degree_ratio: 0 # IGL: 0!
sparsity_ratio: 0 # IGL: 0!
graph_learn_ratio: 0 # 0
input_graph_knn_size: 400 # weighted_cosine: IGL: 400!
graph_learn_hidden_size: 70 # kernel: 90, attention: 70
graph_learn_epsilon: 0.7 # IGL: 0.7!
graph_learn_topk: null #
# graph_learn_hidden_size2: 70 # kernel: 90, attention: 70
# graph_learn_epsilon2: 0 # weighted_cosine: 0
# graph_learn_topk2: null # attn-GCN: 140: 64.1, kernel-GCN: 100
graph_learn_num_pers: 4 # IGL: 4!
graph_hops: 2

# GAT only
gat_nhead: 8
gat_alpha: 0.2


# Training
optimizer: 'adam' # best
learning_rate: 0.001 # adam: 0.001
weight_decay: 0 # adam: 0
lr_patience: 2
lr_reduce_factor: 0.5 # GCN: 0.5
grad_clipping: null # null
grad_accumulated_steps: 1
eary_stop_metric: 'r2' # r2
pretrain_epoch: 0 #
max_iter: 10
eps_adj: 3.e-2 # IGL: 3.e-2!


# Text data only
batch_size: 16 # 16
data_split_ratio: '0.6,0.2,0.2'
fix_vocab_embed: True
word_embed_dim: 300
top_word_vocab: 10000
min_word_freq: 10
max_seq_len: 1000
word_dropout: 0.5 # 0.5
rnn_dropout: 0.5 # 0.5
no_gnn: False



random_seed: 1234
shuffle: True # Whether to shuffle the examples during training
max_epochs: 1000
patience: 10
verbose: -1
print_every_epochs: 1 # Print every X epochs


# Testing
out_predictions: True # Whether to output predictions
save_params: True # Whether to save params
logging: True # Turn it off for Codalab


# Device
no_cuda: False
cuda_id: 0
